\documentclass{oblivoir}

\usepackage{fapapersize, amssymb, amsmath, graphicx, subcaption}
\usepackage{booktabs}
\usepackage[dvipsnames, svgnames]{xcolor}
\usepackage[skins]{tcolorbox}
\usepackage{mathtools}

\usefapapersize{*,*,30mm,*,30mm,*}

\usepackage{fancyhdr}

\pagestyle{fancy}
\fancyhf{}
\renewcommand{\headrulewidth}{4pt}
\renewcommand{\footrulewidth}{2pt}
\fancyhead[LE,RO]{}
\fancyhead[RE,LO]{}
\fancyfoot[CE,CO]{\thepage}

\newcommand{\comb}[2]{{}_{#1}\mathrm{C}_{#2}}
\newcommand{\perm}[2]{{}_{#1}\mathrm{P}_{#2}}
\newcommand{\repe}[2]{{}_{#1}\mathrm{H}_{#2}}
\newcommand{\DC}[1]{\textcolor{DarkMagenta}{#1}}% prints in DarkCyan
\newcommand{\UP}[1]{$^{\mbox{\DC{\footnotesize #1}}}$}
\newcommand\iidsim{\stackrel{\mathclap{iid}}{\sim}}
\newcommand{\mat}[2]{\begin{pmatrix} #1 \\ #2 \end{pmatrix}}


\newtcolorbox{myframe}[2][]{%
  enhanced,colback=white,colframe=black,coltitle=black,
  sharp corners,boxrule=0.4pt,
  fonttitle=\itshape,
  attach boxed title to top left={yshift=-0.3\baselineskip-0.4pt,xshift=2mm},
  boxed title style={tile,size=minimal,left=0.5mm,right=0.5mm,
    colback=white,before upper=\strut},
  title=#2,#1
}


\begin{document}
\title{\LARGE\bfseries 고급수리통계학 \\  \large Note. }
\author{\large Oh Jae Kwon \medskip \\{\normalsize 충북대학교 통계학과}}
\maketitle

\tableofcontents
\newpage

\chapter{\LARGE Some Special Distributions}

\section{확률변수, 분포}
\textbf{확률변수}\UP{Random variable}

\begin{itemize}
\item 확률변수는 확률실험\UP{random experiment}을 했을 때 발생할 수 있는 결과들을 숫자로 요약 해주는 \textbf{대응규칙(함수)}
$$
X : s \rightarrow \mathbb{R}, \quad  s \in S
$$
\item $s$ : 표본공간의 하나의 원소, $S$ : 표본공간
\item 확률질량함수\UP{probability mass function}는 이산형 확률변수가 가질 수 있는 값 $x_1, x_2, \ldots$을 취할 확률을 계산해주는 함수
$$
p_X (x) = P(X=x), \quad x = x_1, x_2, \ldots
$$
\item $p_X$ 확률변수 $x$에 대한 pmf
\item $P : A \rightarrow [0, 1]$, P(사건, 집합)
\item[$\star\star\star$] $X = x$의 의미는 사건(event)이다. : $\{s : X(s) = x, s \in S\}$
\item 누적확률함수\UP{cumulative distribution function}는 확률변수가 특정값 또는 그 이하의 값을 취할 확률을 계산해주는 함수
$$
F_X(x) = P(X \leq x), \quad x \in \mathbb{R}
$$
$<$ 특징 $>$
\begin{enumerate}
\item 비감소함수
\item 
$
\displaystyle
\lim_{x \rightarrow -\infty} F_X(x) = 0 , \quad
\lim_{x \rightarrow \infty} F_X(x) = 1
$
\item
$
\displaystyle
\lim_{x \rightarrow  x_0^+} F_X(x) = x_0 \qquad \mbox{(right continuous)}
$
\end{enumerate}
\end{itemize}
\textbf{분포}\UP{distribution}
\begin{itemize}
\item 확률변수가 가질 수 있는 값들이 흩어져 있는 정도
\item 확률변수 $X$의 분포를 구하기
\begin{itemize}
\item[$\circ$] 확률변수 $X$의 의미
\item[$\circ$] 확률변수 $X$가 가질수 있는 값
\item[$\circ$] probability mass function, \textbf{check!} sum to 1 \& cumulative distribution function
\item[$\circ$] $E(X), \; Var(X)$
\item[$\circ$] moment generating function
\end{itemize}
\end{itemize}

\section{베르누이 분포, 이항 분포}

\textbf{베르누이 시행}\UP{Bernoulli trial}
\begin{itemize}
\item 표본공간이 \textbf{오직 두 개의 상호배타적인 원소로 구성된 실험}에서 시행

\begin{myframe}{\footnotesize 참고}
임의로 표본공간을 두개의 원소만을 가질 수 있게 나눠서 확률변수를 정의할 경우
\begin{itemize}
\item 확률변수 X는 Bernoulli 확률변수이다.
\item But, Bernoulli trial 은 아니다!
\item[ex)] 주사위 1개를 던지는 실험 $S = {1,2,3,4,5,6}$
$$
X = 
\begin{cases}
1, \quad \mbox{홀수} \\
0, \quad \mbox{짝수}
\end{cases}
$$
X는 Bernoulli 확률변수 이지만 Bernoulli trial 은 아니다. 
\end{itemize}
\end{myframe}
\end{itemize}
\textbf{베르누이 확률변수} $X \sim Bern(p)$
\begin{itemize}
\item 확률변수 $X$가 취할 수 있는 값이 오직 1 또는 0
\item $P(X=1) = p, \quad P(X=0) = 1-p, \quad 0 \leq p \leq 1$
\end{itemize}
\textbf{베르누이 확률변수의 확률질량함수}

1.
$$
p_X(x) = 
\begin{cases}
p, \quad &x = 1 \\
1-p, \quad &x = 0 \\
0, \quad &o.w
\end{cases}
$$

2.
$$
p^x(1-p)^{1-x}, \quad x = 0, 1
$$
\begin{itemize}
\item $x = 0,1$ : support, $x$가 가질 수 있느느 값 이외에는 0값을 갖는다.
\end{itemize}
\textbf{독립인 두 베르누이 확률변수의 합}
$$
X_1, X_2 \iidsim Bern(p)
$$
$iid$ : 동일하며\UP{identically} 서로 독립인\UP{independent} 분포\UP{distributed}를 따른다.
\begin{myframe}{독립성\UP{independence}}
$$
P_{X_1,X_2}(x_1,x_2) = P_{X_1}(x_1)P_{X_2}(x_2)
$$
\end{myframe}
\begin{itemize}
\item[Q.] 확률변수 $X$를 $X = X_1 + X_2$라 하자.
\item $X$가 가질 수 있는 값은?  $0,1,2$
\item $X$의 확률질량함수는?
\begin{align*}
P(X = 0) &= P(X_1 = 0, X_2 = 0) = P(X_1 = 0) P(X_2 = 0) = (1-p)^2 \\
P(X = 1) &= P(X_1 = 1, X_2 = 0) + P(X_1 = 0, X_2 = 1)  \\
&= P(X_1 = 1) P(X_2 = 0) + P(X_1 = 0) P(X_2 = 1)\\
&= p(1-p) + p(1-p) = 2p(1-p) \\
P(X = 2) &= P(X_1 = 1, X_2 = 1) = P(X_1 = 1) P(X_2 = 1) = p^2 
\end{align*}
\item $X$의 의미 : 성공 횟수
\end{itemize}
\textbf{이항 분포}\UP{binomial distribution}

성공확률 $p$인 베르누이 시행을  n번 독립적으로 반복시행하는 경우, 확률변수 $X$를 `성공 횟수'라고 하자. $X$는 
시행 횟수 $n$, 성공확률$p$를 모수\UP{parameter}로 갖는 이항 분포를 따른다.
$$
X \sim Bin(n,p)
$$
$$
X_1, X_2, \ldots, X_n \iidsim Bern(p),\quad X = X_1 + \cdots + X_n \sim Bin(n,p)
$$
\begin{itemize}
\item $X$가 가질 수 있는 값 : $0, \cdots, n$
\item 베르누이 확률변수와 이항 확률변수와의 관계
$$
Bin(1,p) \equiv Bern(p)
$$
\end{itemize}
\textbf{이항 확률변수의 확률질량함수}
\begin{itemize}
\item $p_X(k) = P(X=k) = \mat{n}{k}p^x (1-p)^{n-k}, \quad k = 0,1,\ldots, n$
\item 확률질량함수는
$$
\displaystyle
\sum_{k=0}^n p_X(k) = 1
$$
을 반드시 만족해야한다.
\end{itemize}
\begin{myframe}{이항 정리\UP{binomial theorem}}
$$
\displaystyle
(a+b)^n = \sum_{j=0}^n \mat{n}{j} a^j b^{n-j}
$$
\end{myframe}
\textbf{독립인 이항확률변수의 합의 분포}
$$
X \sim Bin(n,p),\quad Y \sim Bin(m,p),\quad (n,m\mbox{은 자연수})
$$
$$
Z = X + Y \sim Bin(n+m, p)
$$
$Z$의 확률질량함수
\begin{align*}
P_Z(k) &= P(Z = k) = P(X+Y = k) = \sum_{j=0}^m P(X = k-j | Y = j)P(Y=j) \\
& = \sum_{j=0}^m P(X = k-j)P(Y=j) = \sum_{j=0}^m \mat{n}{k-j} p^{k-j} (1-p)^{n-k+j} \mat{m}{j} p^j (1-p)^{m-j} \\
& = \sum_{j=0}^m \mat{n}{k-j} \mat{m}{j} p^k (1-p)^{n+m-k} = p^k (1-p)^{n+m-k} \sum_{j=0}^m  \mat{n}{k-j} \mat{m}{j} \\
& = \mat{n+m}{k} p^k (1-p)^{n+m-k}
\end{align*}
\begin{myframe}{전확률 공식\UP{Law of Total Probability}}
\begin{align*}
P(A) &= P((A \cap B_1) \cup \cdots \cup (A \cap B_J)) \\
&= \sum_{j=1}^J P(A \cap B_j) = \sum_{j=1}^J P(A|B_j)P(B_j)
\end{align*}
\end{myframe}
\begin{myframe}{\DC{Vandermonde convolution}}
$$
\mat{n+m}{k} = \displaystyle \sum_{j=0}^{k} \mat{n}{j} \mat{m}{k-j}
$$
\end{myframe}

\section{기대값, 지시확률변수, 초기하 분포}
\textbf{예제} : 52장의 트럼프 카드에서 임의로 5장을 뽑는 실험
\begin{itemize}
\item 확률변수 $X$ : Ace의 개수
\item 확률변수 $X$가 갖을 수 있는 값 : 0, 1, 2, 3, 4
\item $X$의 확률질량함수 - 비복원
$$
P(X=k) = \frac{\mat{4}{k} \mat{48}{5-k}}{\mat{52}{5}}
$$
\item ex) 초기하 (Hypergeometry) - 복원
\begin{align*}
&Y_1, Y_2, Y_3, Y_4, Y_5 \sim Bern(\frac{4}{52}) \\
&Y_j \triangleq 
\begin{cases}
1\quad ,Ace  \\
0\quad ,Not Ace
\end{cases} \\
&Y = \sum_{j=1}^5 Y_i \sim Bin(5, \frac{4}{52}) \\
&P_Y(k) = \mat{5}{k} \left(\frac{4}{52} \right)^k \left(\frac{48}{52} \right)^{5-k}
\end{align*}
\end{itemize}
\textbf{기대값} : 이산형 확률변수 $X$의 기대값은 $X$가 가질 수 있는 모든 값들에 각각의 확률을 가중치로 하여 계산된 산술평균
$$
E(X) = \sum_x xP(X=x) = \sum_x p_x(x)
$$
\begin{itemize}
\item 이산형 확률변수의 기대값을 계산하려면 pmf가 필요
\item 확률변수의 기대값 $E(X)$은 고정된 상수\UP{constant}이다. (randomness가 없음)
\end{itemize}
\textbf{베르누이, 이항분포의 기대값}
\begin{itemize}
\item \textbf{Bernoulli} : $X \sim Bern(p)$일때, $E(X)$ ?
$$
0 \cdot P(X=0) + 1 \cdot P(X=1) = p
$$
\item \textbf{Binomial} : $X \sim Bin(n,p)$일때, $E(X)$ ?
\begin{align*}
\sum_{x=0}^n x \mat{n}{x} p^x (1-p)^{n-x} &= \sum_{x=1}^n n\mat{n-1}{x-1}p^x(1-p)^{n-x} \\
&= n\sum_{x=1}^n \mat{n-1}{x-1}p^x(1-p)^{n-x} \\
&= n\sum_{t=0}{n-1} \mat{n-1}{t}p^{t+1}(1-p)^{(n-1)-t} \\
&= np\sum_{t=0}{n-1} \mat{n-1}{t}p^{t}(1-p)^{(n-1)-t} = np
\end{align*}
\end{itemize}

\begin{myframe}{공식}
$n$개 중 $k$개를 선택하여 대표를 뽑는 경우의 수와 대표 한 명을 먼저 뽑고 나머지 $(n-1)$명 중에서 $(k-1)$명을 뽑는 경우의 수
$$
k \mat{n}{k} = n \mat{n-1}{k-1}
$$
\end{myframe}
\textbf{기댓값의 선형성}\UP{linearity}

기대값의 성질 : 임의의 상수 $a, b$와 확률변수 $X, Y$에 대하여
\begin{enumerate}
\item $E(a) = a$
\item $E(bX) = bE(X)$
\item $E(a+bX) = a + bE(X)$
\item $E(X + Y) = E(X) + E(Y)$, ($\star$ $X,Y$의 독립성과 무관) 
\item $E(aX + bY) = aE(X) + bE(Y)$, ($\star$ $X,Y$의 독립성과 무관)
\end{enumerate}
\textbf{지시확률변수}\UP{indicator random variable}
\begin{itemize}
\item 1 또는 0을 갖는 확률변수(베르누이 확률변수와 매우 유사)
\item 복잡한 문제를 우리가 관심있는 사건과 그 나머지에만 관심을 둔다면, 어떤 문제든지 지시확률변수를 이용해 표현이 가능
$$
X = 
\begin{cases}
1, \quad if \; A\mbox{라는 사건이 발생}\\
0, \quad o.w
\end{cases}
$$
\item 지시확률변수를 잘 이용하면 관심있는 사건에 대한 확률을 기대값으로 표현이 가능 $\Rightarrow$ \textbf{fundamental bridge}
$$
E(X) = P(A)
$$
\end{itemize}
\textbf{초기하분포의 기대값}
\begin{itemize}
\item $X \sim Hypergeom(n, K, N)$
\item 단, $n<N, \; K < N, \; K < n$
\item pmf
$$
P(X=k) = \frac{\mat{K}{k} \mat{N-K}{n-k}}{\mat{N}{n}}, \quad k = 0,\cdots,K
$$
\item $X$의 기대값 $E(X)$ ?
\begin{itemize}
\item[1.] pmf를 이용
\begin{align*}
E(X) &= \sum_{k=0}^K kP(X=k) = \sum_{k=1}^K k \frac{\mat{K}{k}\mat{N-K}{n-k}}{\mat{N}{n}}\\
&= \sum_{k=1}^K K \frac{\mat{K-1}{k-1}\mat{N-K}{n-k}}{\mat{N}{n}}\\
&= \sum_{k=1}^K \frac{Kn}{N} \frac{\mat{K-1}{k-1}\mat{(N-1)-(K-1)}{(n-1)-(k-1)}}{ \mat{N-1}{n-1}}\\
&= n \cdot \frac{K}{N}
\end{align*}
\begin{myframe}{공식}
$$
\mat{n}{r} = \frac{n!}{r!(n-r)!} = \frac{n}{r} \cdot \frac{(n-1)!}{(r-1)!((n-1)-(r-1))!} = \frac{n}{r} \cdot \mat{n-1}{r-1}
$$
\end{myframe}
\item[2.] 선형성, 지시확률변수를 이용
\begin{align*}
&Y_1, Y_2, Y_3, Y_4, Y_5 \sim Bern(\frac{4}{52}) \\
&Y_j \triangleq 
\begin{cases}
1\quad ,Ace  \\
0\quad ,Not Ace
\end{cases} \\
&Y = \sum_{j=1}^5 Y_i \sim Bin(5, \frac{4}{52}) \\
E(Y) &= E(Y_1 + \cdots + Y_5) = E(Y_1) + \cdots + E(Y_5) \\
&= \sum_{j=1}^5 P(\mbox{$j$th card Ace}) = 5 \cdot \frac{4}{52}
\end{align*}
\end{itemize}
\end{itemize}

\section{기하 분포}
\textbf{기하 분포}\UP{geometric distribution}

성공확률 $p$인 베르누이 시행을 독립적으로 반복시행하는 경우, 확률변수 $X$를 `한번 성공할때까지 시행한 횟수' 라고 하자.
이 때 $X$는 성공확률 $p$를 모수로 갖는 기하 분포를 따른다고 한다.
$$
X \sim Geom(p)
$$
\begin{itemize}
\item $X$가 가질 수 있는 값은? $1, 2, 3, \cdots, \infty$
\item $X$의 pmf? $P_X(k) = P(X=k) = (1-p)^{k-1}p, \quad k = 1, \cdots, \infty$ \par
\textbf{check!}
$$
\sum_{k=1}^{\infty} (1-p)^{k-1}p =  p\sum_{k=1}^{\infty} (1-p)^{k-1} = p \cdot \frac{1}{1-(1-p)} = 1
$$
\end{itemize}
\begin{myframe}{무한등비급수\UP{infinite geometric series}}
$a \neq 0$일때 무한등비급수
$$
a + ar + ar^2 + \cdots = \sum_{n=1}^{\infty} ar^{n-1} = \frac{a}{1-r}
$$
\end{myframe}
\begin{itemize}
\item $X \sim Geom(p)$ 일 때, $E(X)$ ?
\begin{align*}
E(X) &= \sum_{k=1}^{\infty} k P(X=k) = \sum_{k=1}^{\infty} k (1-p)^{k-1} p \\
&= p\sum_{k=1}^{\infty} k (1-p)^{k-1} = p \frac{1}{p^2} = \frac{1}{p}
\end{align*}
\end{itemize}
\begin{myframe}{무한등비급수 변형}
\begin{align*}
 & \quad \sum_{k=1}^{\infty} (1-p)^{k-1} = \frac{1}{p} \\
\Leftrightarrow& \quad \frac{1}{1-p}\sum_{k=1}^{\infty} (1-p)^{k} = \frac{1}{p} \\
\Leftrightarrow& \quad \sum_{k=1}^{\infty} (1-p)^{k} = \frac{1-p}{p} \quad \mbox{양변 $p$에 대하여 미분} \\
\Leftrightarrow& \quad - \sum_{k=1}^{\infty} k(1-p)^{k-1} = -\frac{1}{p^2} \\
\Leftrightarrow& \quad \sum_{k=1}^{\infty} k(1-p)^{k-1} = \frac{1}{p^2}
\end{align*}
\end{myframe}
\begin{itemize}
\item \textbf{Story proof} $p=0.1$이라는 것은 한번 성공할 확률이 $0.1$이다. 다시 말하면 한 번 성공하려면 10번은 시행해야 한다.
\end{itemize}

\section{음이항 분포}
\textbf{음이항 분포  \UP{negative binomial distribution}}

성공확률 $p$인 베르누이 시행을 독립적으로 반복시행하는 경우, 확률변수 $X$를 '$r$번 성공할때까지 시행한 횟수'
$$
X \sim NB(r,p)
$$
\begin{itemize}
\item $X$가 가질 수 있는 값 : $r, r+1, r+2, \ldots$
\item $X$의 pmf ?
$$
P_X(k) = P(X = k) = \mat{k-1}{r-1} p^r (1-p)^{k-r} 
$$
\begin{myframe}{음이항 정리 (음이항 급수)}
$$
(a + b)^{-n} = \sum_{k=0}^{\infty} \mat{-n}{k} a^k b^{-n-k}
$$
Tayler expansion
$$
f(x) = (1-x)^{-n}
$$
\begin{align*}
f(x) =& f(0) + f^{\prime}(0) x + \frac{1}{2} f^{\prime\prime}(0) x^2 + \frac{1}{3!} f^{\prime\prime\prime}(0) x^3 + \cdots \\
=& 1 + (-n)(-1) x + \frac{1}{2} (-n)(-n-1)(-1)(-1) x^2 \\
& + \frac{1}{3!} (-n)(-n-1)(-n-2)(-1)(-1)(-1) x^3 + \cdots \\
=& \sum_{k=0}^{\infty} \mat{-n}{k} (-1)^k x^k = \sum_{k=0}^{\infty} \mat{n+k-1}{k} x^k 
\end{align*}
\end{myframe}
\begin{align*}
\sum_{k=0}^{\infty} \mat{k-1}{r-1} p^r (1-p)^{k-r}  =& p^r  \sum_{k=0}^{\infty} \mat{k-1}{r-1} (1-p)^{k-r} \\
=& p^r \sum_{r=0}^{\infty} \mat{r+t-1}{r-1} (1-p)^t \\
=&  p^r \sum_{r=0}^{\infty} \mat{r+t-1}{t} (1-p)^t \\
=& p^r(1-(1-p))^-r = p^r p^{-r} = 1
\end{align*}
\item 기하분포와의 관계
$$
X_1, \dots, X_r \sim Geom(p), \quad X = \sum_{i=1}^r X_i \sim NB(r, p)
$$
\end{itemize}

\section{포아송 분포}
\textbf{자연상수 $e = 2.71828$}

자연상수를 정의하는 방법
\begin{enumerate}
\item $x=1$부터 $x = a$까지 $y = \frac{1}{x}$ 의 적분값이 1이 되게하는 $a$ 값
$$
\int_1^e \frac{1}{x} dx = 1
$$
\item
$$
e = \sum_{n=0}^{\infty} \frac{1}{n!} = 1 + \frac{1}{1!} + \frac{1}{2!} + \frac{1}{3!} + \cdots
$$
\item
$$
e = \lim_{t \rightarrow 0} (1 + t)^{\frac{1}{t}} \quad or \quad e = \lim_{n \rightarrow \infty} \left ( 1 + \frac{1}{n} \right)^n
$$
\item 성질
$$
e^a = \lim_{n \rightarrow \infty} \left ( 1 + \frac{a}{n} \right)^n
$$
\end{enumerate}
\textbf{포아송 분포 \UP{poisson}}

단위시간(공간) 에서 발생하는 사건의 발생횟수
\begin{itemize}
\item 시행횟수 $n$ 에는 제약을 두지 않음 (관심이 없음) : $n \rightarrow \infty$
\item (대체적으로) 사건이 발생할 확률 $p$가 아주 작은 경우에 사용됨 : $p \rightarrow 0$
\end{itemize}
$\lambda = np$ 를 단위시간(공간) 평균 발생횟수라고 하면, 확률변수 $X$ 는 모수가 $\lambda$인 포아송 분포를 따른다고 한다.
$$
X \sim P(\lambda)
$$

\textbf{이항분포의 포아송 근사 \UP{approximation}}

$X \sim Bin(n,p)$ 이면
$$
P_X(x) = \mat{n}{x}p^x (1-p)^{n-x}, \quad x = 0,1,2,3,\ldots,n
$$
이때 $n \rightarrow \infty, p \rightarrow 0, \lambda = np(\lambda$ is fixed) 인 경우?
\begin{align*}
p_X(x) &= \frac{n(n-1)\cdots(n-x+1)}{x!} \left (\frac{\lambda}{n} \right)^x \left(1-\frac{\lambda}{n} \right)^{n-x} \\
&= \frac{\lambda^x}{x!} e^{-\lambda} = \frac{\lambda^x e^{-\lambda}}{x!}, \quad x = 0,1,2,\ldots
\end{align*}
기대값
$$
E(x) = \sum_{x=0}^{\infty} x \frac{\lambda^x e^{-\lambda}}{x!} = \lambda \sum_{x=1}^{\infty} \frac{\lambda^{x-1} e^{-\lambda}}{(x-1)!}
= \lambda \sum_{t=0}^{\infty} \frac{\lambda^{t} e^{-\lambda}}{(t)!} = \lambda
$$
\begin{myframe}{$e$의 성질}
$$
\sum_{t = 0}^{\infty} \frac{\lambda^t}{t!} = e^{\lambda}
$$
\end{myframe}
\textbf{증명 : Linearity} $E(X + Y) = E(X) + E(Y)$
\begin{myframe}{}
$$
E(X) = \sum_x x P(X=x) \Rightarrow \sum_s X_{(s)} P(\{s\})
$$
\end{myframe}
\begin{align*}
E(X + Y) &= \sum_s (X+Y)_{(s)} P(\{s\}) = \sum_s (X_{(s)}+Y_{(s)}) P(\{s\}) \\
&=\sum_s X_{(s)} P(\{s\}) + \sum_s Y_{(s)} P(\{s\}) = E(X) + E(Y)
\end{align*}

\section{분산, LOTUS}
$$
Var(X) = E[(X - E(X))^2] = E(X^2) - \{E(X)\}^2 \geq 0 \quad \leftrightarrow \quad E(X^2) \geq \{E(X)\}^2 
$$

\textbf{Law Of The Unconscious Statistician (LOTUS)}

이산형 확률변수 $X$의 함수꼴인 확률변수 $g(X)$의 기대값 $E[g(X)]$
$$
E[g(X)] = \sum_x g(x)p_X(x) 
$$

$Y = g(x), X = g^{-1}(y)$
\begin{align*}
E(Y) &= \sum_y y P(g(x) = y) = \sum_y y \sum_{x \in g^{-1}(y)} f(x) \\
&= \sum_y \sum_{x \in g^{-1}(y)} g(x) f(x) = \sum_x g(x)f(x)
\end{align*}

\textbf{이산형분포들의 분산 계산}
$$
Var(X) = E(X^2) - \{E(X)\}^2
$$
\begin{itemize}
\item 베르누이 분포 $X \sim Bern(p)$
\item 이항 분포 $X \sim Bin(n,p)$
\item 초기하 분포 $X \sim Hypergeom(n, K, N)$
\item 기하 분포 $X \sim Geom(p)$
\item 음이항 분포 $X \sim NB(r,p)$
\item 포아송 분포 $X \sim P(\lambda)$
\end{itemize}

\section{연속형 \UP{continuous} 확률변수}

주어진 실수구간 내에 속하는 어떠한 실수값도 가질 수 있는 확률변수

연속확률변수는 취할 수 있는 값이 무한히 많기 때문에, 이 중 어떤 한 실수값을 정확히 취할 확률은 0이다. (측정이 불가!)

\textbf{확률밀도함수 \UP{probability density function, pdf}}

확률변수 $X$가 구간 \UP{interval} $[L, U]$에서 값을 가지는 연속형 확률변수인 경우,
\begin{itemize}
\item 모든 $x \in [L, U]$에 대하여 $f_X(x) \geq 0$
\item 모든 $x n\in [L, U]$에 대하여 $f_X(x) = 0$
\item 값을 가지는 모든 구간에서 $f_X$를 적분 \UP{intergration} 하면 1
$$
\int_L^U f_X(x) dx = 1
$$
\item $L \leq a \leq b \leq U$에 대하여
$$
P(a \leq X \leq b) = \int_a^b f_X(x) dx
$$
\end{itemize}

위 조건들을 만족하면 $f_X(x)$를 $X$의 확률밀도함수라 한다.

\textbf{확률밀도함수의 성질}

\begin{itemize}
\item[1.] $f_X(x) \geq 0$, $\int_{-\infty}^{+\infty} f_X(x) dx = 1$
\item[2.] $P(x_1 \leq X \leq x_2) = \int_{x_1}^{x_2} f_X(t)dt$
\item[3.] 함수값 $f_X(x)$가 갖는 의미는?
\item[4.] 연속형 확률변수의 누적밀도함수
$$
F_X(x) = P(X \leq x) = \int_{-\infty}^x f_X(t)dt
$$
\end{itemize}

\section{균등분포\UP{Uniform distribution}}
\begin{itemize}
\item 연속형 확률변수 $X$ 는 실구간 $[a,b]$에서 값을 가지는 연속형 확률변수
\end{itemize}
$$
f_X(x) = 
\begin{cases}
\frac{1}{b-a} \quad &\mbox{if} \quad a\leq x \leq b, \\
0 &o.w
\end{cases}
$$
\begin{align*}
\int_a^b c dx = cb - ca = 1, \\
c = \frac{1}{b-a}
\end{align*}
\begin{align*}
E(X) = \int_a^b x \frac{1}{b-a} dx = \frac{b^2 - a^2}{2(b-a)} = \frac{b+a}{2}
\end{align*}

\textbf{확률적분변환\UP{probability integral transformation}}
\begin{itemize}
\item 목적 : 확률변수 $X$를 생성\UP{generation}하고 싶음
\item 주어진 것
\begin{itemize}
\item 균등분포 $U \sim Unif(0,1)$에서 관측된 값 $u_1,\ldots,u_n$
\item 확률변수 $X$ 의 cdf $F_X$
\item 단, $F_X$는 연속이며 순증가\UP{strictly increasing} 함수 $\rightarrow$ 역함수 존재
$$
F^{-1}_X(U) \overset{d}{\equiv} X
$$
\item 즉, 확률변수 $X$는 $F_X(u_1),\ldots,F_X(u_n)$으로 생성가능
\end{itemize}
\end{itemize}
$$
F^{-1}_X(U) \overset{d}{\equiv} X
$$

$$
F_X(x) = P(X \leq x) = P(F^{-1}(U) \leq x) = P(U \leq F_X(x)) = F_X(x)
$$

\section{정규분포\UP{normal distribution}}
\begin{itemize}
\item 표준정규분포 $Z \sim N(0,1)$의 pdf
$$
f_z(z) = \frac{1}{\sqrt{2\pi}} exp \left( -\frac{z^2}{2} \right)
$$
\item $Z$의 cdf를 $\Phi$라 하면
$$
\Phi (z) = P(Z \leq z),\; \Phi^{\prime}(z) = f_Z(z)
$$
\item 평균 $\mu$, 표준편차 $\sigma$인 정규분포 $X \sim N(\mu, \sigma^2)$와 $Z$의 관계
$$
X = \mu + \sigma Z
$$
\item $X$의 cdf
$$
P(X \leq x) = P(\mu + \sigma Z \leq x) = P( Z \leq \frac{x - \mu}{\sigma} = \Phi \left(\frac{x - \mu}{\sigma} \right)
$$
\item $X$의 pdf 는 cdf를 미분하면
$$
f_X(x) = \Phi^{\prime} \left(\frac{x - \mu}{\sigma} \right) \frac{1}{\sigma} = f_z \left(\frac{x-\mu}{\sigma} \right) \frac{1}{\sigma}
= \frac{1}{\sqrt{2\pi}\sigma} exp \left(- \frac{(x - \mu)^2}{2\sigma^2} \right)
$$
\end{itemize}

\section{지수분포\UP{exponential distribution}}

\begin{itemize}
\item 단위시간당 사건의 편균 발생횟수를 $\lambda$라 할 때, 함번 사건이 일어날때까지 걸리는 시간을 $X$라 하면
$$
X \sim Exp(\lambda)
$$
\end{itemize}
\textbf{포아송과정\UP{poisson process}}
\begin{itemize}
\item $\lambda$ : 단위시간당 사건의 평균 발생횟수 $[0,1]$
\item $\lambda x$ : 어떤 특정 시점 $x > 0$까지 평균 사건 발생횟수 $[0,x]$
\item $X$의 cdf
\begin{align*}
F_X(x) &= P(X \leq x) = 1 - P(X \geq x) \\
&= 1 - P([0,x] \mbox{동안 사건이 일어나지 않음})\\
&= 1 - P(Y = 0) \quad, Y \sim Poisson(\lambda x) \\
&= 1 - \frac{e^{-\lambda x} (\lambda x )^0}{0!} \\
&= 1 - e^{- \lambda x}
\end{align*}
\item $X$의 pdf
$$
f_X(x) = F_X^{\prime}(x) = \lambda e^{- \lambda x} = \frac{1}{\theta} e^{- \frac{x}{\theta}}
$$
\item $\theta = \frac{1}{\lambda}$ : 사건이 한번 발생할 때까지 걸리는 평균 시간
\item $Y \sim Exp(1)$의 기대값과 분산
\begin{align*}
E(Y) &= \int_0^{\infty} y e^{-y} dy = y - \frac{1}{y} e^{-y} |_0^{\infty} = 1 \\
E(Y^2) &= \int_0^{\infty} y^2 e^{-y} dy = 2 \\
Var(Y) &= 1
\end{align*}
\item $Y = \lambda X$, $X = \frac{Y}{\lambda}$
$$
E(X) = \frac{1}{\lambda} = \theta, \quad Var(X) = \frac{1}{\lambda^2} = \theta^2
$$
\item $X \sim Exp(\lambda)$
\end{itemize}
\begin{myframe}{무기억성\UP{memoryless property}}
지수분포, 기하분포에서 나타나는 성질
\begin{align*}
P(X \geq s+t | X \geq s) &=\frac{P(X \geq s+t, X \geq s)}{P(X \geq s)} = \frac{P(X \geq s+t)}{P(X \geq s)} \\
&= \frac{1 - F_X(s+t)}{1 - F_X(s)} = \frac{1 - (1-e^{-\lambda (s+t)})}{1 - (1 - e^{-\lambda s})}\\
&= \frac{e^{-\lambda (s+t)}}{e^{-\lambda s}}  = e^{-\lambda t} = 1 - (1 - e^{-\lambda t} \\
&= 1 - F_X(t) = P(X > t)
\end{align*}
\end{myframe}

\section{적률생성함수\UP{moment generating function, MGF}}
\begin{itemize}
\item 통계학에서 적률\UP{moment} : 
양수 $n$에 대하여 $E[X^n]$을 확률변수 $X$의 $n$차 적률\UP{n - th moment}이라 한다.
\item 기대값 : 1차 적률, 분산 : 2차 적률과 관계
\item 확률변수 $X$의 적률생성함수 $M_X$는 다음으로 정의
$$
M_X(t) = E[e^{tX}]
$$
\item $M_X$가 왜 적률생성함수일까?
$$
M_X(t) = E[e^{tX}] = E \left( \sum_{n=0}^{\infty} \frac{X^n t^n}{n!} \right) = \sum_{n=0}^{\infty} \frac{t^n}{n!} E(X^n)
$$
\begin{myframe}{기하 급수\UP{geometric series}}
$$
e = \sum_{n=0}^{\infty} \frac{1}{n!} \quad  \Rightarrow \quad e^{ax} = \sum_{n=0}^{\infty} \frac{a^n x^n}{n!}
$$
\end{myframe}
\begin{align*}
M_X(t) &= \sum_{n=0}^{\infty} \frac{t^n}{n!} E(X^n) \\
&= \frac{t^0 E(X^0)}{0!} + \frac{t^1 E(X^1)}{1!} + \frac{t^2 E(X^2)}{2!} + \cdots \\
M_X^{\prime} (t) &= E(X) + t \times \star + t^2 \times \star \\
M_X^{\prime} (0) &= E(X) \\
M_X^{\prime\prime} (t) &= 0 + E(X^2) + t \times \star \\
M_X^{\prime\prime} (0) &= E(X^2) \\
&\vdots \\
M_X^{(n)} (0) &= E(X^n)
\end{align*}
\item $M_X$를 $t=0$을 포함하는 구간에서만 정의하면 적률을 구할수 있음
\end{itemize}

\textbf{적률생성함수 중요성}
\begin{itemize}
\item MGF $M_X$는 모든 실수에서 정의될 필요까지 없이, 0을 포함하는 정당한 구간 $(-a,a), a>0$에서만 정의가 되면 된다.
\item 확률변수 $X$의 $n$차 적률 $E[X^n]$은 적률생성함수를  $n$번 미분하여 얻을 수 있음
$$
E[X^n] = M_X^{(n)}(0)
$$
\item 적률생성함수는 확률변수의 분포를 결정한다. 즉, 두 확률변수의 MGF가 같으면 분포가 같음을 의미
\item 두 확률변수 $X,Y$의 MGF가 각각 $M_X, M_Y$ 이고 $X$와 $Y$ 가 독립이면, 확률변수 $Z = X + Y$의 MGF $M_Z$는 다음과 같이 얻어짐
$$
M_Z(t) = E[e^{t(X+Y)}] = E[e^{tX}e^{tY}] = E[e^{tX}]E[e^{tY}] =  M_X(t) M_Y(t)
$$
\end{itemize}

\textbf{예제 : 적률생성함수}
\begin{itemize}
\item $X \sim Bern(p)$, $X = 0,1$
\begin{align*}
M_X(t) &= E[e^{tx}] = \sum_x e^{tx} P(X = x) = e^0 (1-p) + e^t p = (1-p) + e^t p \\
M_X^{\prime}(t) &= pe^t, \quad M_X^{\prime}(0) = p \\
M_X^{\prime\prime}(t) &= pe^t, \quad M_X^{\prime\prime}(0) = p \\
Var(X) &= E[X^2] - \{ E[X] \}^2 = p - p^2 = (1-p)p
\end{align*}
\item $X \sim Bin(n,p)$, $X = 0,1,2,\ldots, n$
\begin{align*}
M_X(t) &= E[e^{tx}] = \sum_x e^{tx} P(X = x) = \sum_{x=0}^n e^{tx} \mat{n}{x} p^x (1-p)^{n-x}\\
&= \sum_{x=0}^n \mat{n}{x} (pe^t)^x (1-p)^{n-x} = (pe^t + (1-p))^n, \quad \mbox{이항정리}\\
M_X^{\prime}(t) &= npe^t(pe^t + (1-p))^{n-1}, \quad M_X^{\prime}(0) = np \\
M_X^{\prime\prime}(t) &= n(n-1)p^2e^{2t}(pe^t + (1-p))^{n-2} + npe^t(pe^t + (1-p))^{n-1}, \\
 M_X^{\prime\prime}(0) &= n^2p^2 - np^2 + np \\
Var(X) &= E[X^2] - \{ E[X] \}^2 = np(1-p)
\end{align*}
\item$X \sim Exp(1)$
\begin{align*}
M_X(t) &= E[e^{tx}] = \int_0^{\infty} e^{tx} e^{-x} dx = \int_0^{\infty} e^{-x(1-t)}dx  = (1-t)^{-1}, \quad |t| \leq 1\\
M_X^{(n)}(t) &= n!(1-t)^{-n-1} = E(X^n)\\
M_X(t) &= \frac{1}{1-t} = \sum_{n=0}^{\infty} t^n = \sum_{n=0}^{\infty} \frac{n!t^n}{n!} = \sum_{n=0}^{\infty} \frac{E(X^n)t^n}{n!} \\
E(X^n) &= n! 
\end{align*}
\item $Y \sim Exp(\lambda)$, $X \sim Exp(1)$
$$
E(Y^n) = E\left(\frac{X^n}{\lambda^n} \right) = \frac{1}{\lambda^n} E(X^n) = \frac{n!}{\lambda^n} = \theta^n n!
$$
\item $Z \sim N(0,1)$
\begin{align*}
M_X(t) &= E[e^{tx}] = \int_{-\infty}^{\infty} e^{tz} \frac{1}{\sqrt{2\pi}} e^{- \frac{z^2}{2}} dz \\
&= \frac{1}{\sqrt{2\pi}} \int_{-\infty}^{\infty} e^{-\frac{z^2}{2} + tz} dz 
=  \frac{1}{\sqrt{2\pi}} \int_{-\infty}^{\infty} e^{-\frac{(z-t)^2}{2}} e^{\frac{t^2}{2}} dz \\
&= e^{\frac{t^2}{2}}  \int_{-\infty}^{\infty} \frac{1}{\sqrt{2\pi}} e^{-\frac{(z-t)^2}{2}} dz = e^{\frac{t^2}{2}}
\end{align*}
\begin{itemize}
\item $E(Z^n)$
\item 홀수
$$
E(Z^n) = \int_{-\infty}^{\infty} z^n \frac{1}{\sqrt{2\pi}} e^{-\frac{z^2}{2}} dz = 0
$$
$Z_n$ 기함수로 전구간에 적분을 하면 0 이된다. 

$\frac{1}{\sqrt{2\pi}} e^{-\frac{z^2}{2}}$ 우함수 : $y$축 대칭

기함수 $\times$ 우함수 = 기함수
\item 짝수
$$
M_Z(t) = e^{\frac{t^2}{2}} = \sum_{n=0}^{\infty} \frac{(\frac{t^2}{2})^n}{n!} = \sum_{n=0}^{\infty} \frac{\frac{t^{2n}}{2^n} (2n)!}{n!(2n)!} = \sum_{n=0}^{\infty} \frac{\frac{(2n)!}{2^nn!}t^{2n}}{(2n)!}
$$
\begin{align*}
E(Z^{2n}) &= \frac{(2n)!}{2^n n!} \\
E(Z^2) & = 1 \\
E(Z^4) & = 1 \times 3 \\
E(Z^6) & = 1 \times 3 \times 5
\end{align*}
\end{itemize}
\item $X \sim Poisson(\lambda)$
$$
M_X(t) = E(e^{tx}) = \sum_{x=0}^{\infty} e^{tx} \frac{e^{-\lambda} \lambda^x}{x!} = 
e^{-\lambda}\sum_{x=0}^{\infty} \frac{ (\lambda e^{t})^x}{x!} =
e^{-\lambda} e^{\lambda e^t} = e^{-\lambda + \lambda e^t}
$$
\item $X \sim Poisson(\lambda_1)$, $Y \sim Poisson(\lambda_2)$, $X \bot Y$, $Z = X + Y$
\begin{align*}
M_Z(t) = M_X(t) M_Y(t) = e^{-\lambda_1 + \lambda_1 e^t} e^{-\lambda_2 + \lambda_2 e^t} = e^{-(\lambda_1 + \lambda_2) + e^t (\lambda_1 + \lambda_2)}
\end{align*}
\end{itemize}

\end{document}